{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 212: Data Science Programming 1\n",
    "___\n",
    "\n",
    "### Week 8: Data Wrangling, Aggregation and Group Operations\n",
    "___\n",
    "\n",
    "### Mon., 21, 2018, and Wed., May 23, 2018\n",
    "---\n",
    "\n",
    "**Question:**\n",
    "- What capabilities does Python provide for complex group operations? \n",
    "\n",
    "**Objectives:**\n",
    "- Split a pandas object into pieces using one or more keys (in the form of functions, arrays, or DataFrame column names)\n",
    "- Calculate group summary statistics, like count, mean, or standard deviation, or a user-defined function\n",
    "- Apply within-group transformations or other manipulations, like normalization, linear regression, rank, or subset selection\n",
    "- Compute pivot tables and cross-tabulations\n",
    "- Perform quantile analysis and other statistical group analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "PREVIOUS_MAX_ROWS = pd.options.display.max_rows\n",
    "pd.options.display.max_rows = 20\n",
    "np.random.seed(12345)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', figsize=(10, 6))\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Strings\n",
    "### Regular Expressions\n",
    "Regular expressions provide a flexible way to search or match (often more complex)\n",
    "string patterns in text. A single expression, commonly called a regex, is a string\n",
    "formed according to the regular expression language. Python’s built-in re module is\n",
    "responsible for applying regular expressions to strings.\n",
    "\n",
    "The re module functions fall into three categories: pattern matching, substitution,\n",
    "and splitting. Naturally these are all related; a regex describes a pattern to locate in the\n",
    "text, which can then be used for many purposes. \n",
    "\n",
    "Suppose we wanted to split a string with a variable number of whitespace characters\n",
    "(tabs, spaces, and newlines). The regex describing one or more whitespace characters\n",
    "is \\s+:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import re\n",
    "text = \"foo    bar\\t baz  \\tqux\"\n",
    "re.split('\\s+', text)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call re.split('\\s+', text), the regular expression is first compiled, and\n",
    "then its split method is called on the passed text. You can compile the regex yourself\n",
    "with re.compile, forming a reusable regex object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "regex = re.compile('\\s+')\n",
    "regex.split(text)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, instead, you wanted to get a list of all patterns matching the regex, you can use the\n",
    "findall method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "regex.findall(text)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match and search are closely related to findall. While findall returns all matches\n",
    "in a string, search returns only the first match. More rigidly, match only matches at\n",
    "the beginning of the string. As a less trivial example, let’s consider a block of text and\n",
    "a regular expression capable of identifying most email addresses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "text = \"\"\"Dave dave@google.com\n",
    "Steve steve@gmail.com\n",
    "Rob rob@gmail.com\n",
    "Ryan ryan@yahoo.com\n",
    "\"\"\"\n",
    "pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,4}'\n",
    "\n",
    "# re.IGNORECASE makes the regex case-insensitive\n",
    "regex = re.compile(pattern, flags=re.IGNORECASE)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using findall on the text produces a list of the email addresses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "regex.findall(text)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search returns a special match object for the first email address in the text. For the\n",
    "preceding regex, the match object can only tell us the start and end position of the\n",
    "pattern in the string:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "m = regex.search(text)\n",
    "m\n",
    "text[m.start():m.end()]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regex.match returns None, as it only will match if the pattern occurs at the start of the\n",
    "string:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "print(regex.match(text))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatedly, sub will return a new string with occurrences of the pattern replaced by the\n",
    "a new string:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "print(regex.sub('REDACTED', text))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted to find email addresses and simultaneously segment each\n",
    "address into its three components: username, domain name, and domain suffix. To\n",
    "do this, put parentheses around the parts of the pattern to segment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})'\n",
    "regex = re.compile(pattern, flags=re.IGNORECASE)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A match object produced by this modified regex returns a tuple of the pattern components\n",
    "with its groups method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "m = regex.match('wesm@bright.net')\n",
    "m.groups()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "findall returns a list of tuples when the pattern has groups:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "regex.findall(text)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sub also has access to groups in each match using special symbols like \\1 and \\2. The\n",
    "symbol \\1 corresponds to the first matched group, \\2 corresponds to the second, and\n",
    "so forth:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "print(regex.sub(r'Username: \\1, Domain: \\2, Suffix: \\3', text))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Patterns\n",
    "The power of regular expressions is that they can specify patterns, not just fixed characters. Here are the most basic patterns which match single chars:\n",
    "\n",
    "* a, X, 9, < -- ordinary characters just match themselves exactly. The meta-characters which do not match themselves because they have special meanings are: `. ^ $ * + ? { [ ] \\ | ( )`\n",
    "* . (a period) -- matches any single character except newline '\\n'\n",
    "* \\w -- (lowercase w) matches a \"word\" character: a letter or digit or underbar [a-zA-Z0-9_]. Note that although \"word\" is the mnemonic for this, it only matches a single word char, not a whole word. \\W (upper case W) matches any non-word character.\n",
    "* \\b -- boundary between word and non-word\n",
    "* \\s -- (lowercase s) matches a single whitespace character -- space, newline, return, tab, form `[ \\n\\r\\t\\f]`. \n",
    "* \\S (upper case S) matches any non-whitespace character.\n",
    "* \\t, \\n, \\r -- tab, newline, return\n",
    "* \\d -- decimal digit [0-9] (some older regex utilities do not support but \\d, but they all support \\w and \\s)\n",
    "* ^ = start, $ = end -- match the start or end of the string\n",
    "* \\ -- inhibit the \"specialness\" of a character. So, for example, use \\. to match a period or \\\\ to match a slash. If you are unsure if a character has special meaning, such as '@', you can put a slash in front of it, \\@, to make sure it is treated just as a character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise \n",
    "If a row in a DataFrame belongs to multiple categories, things are a bit more complicated.\n",
    "Let’s look at the MovieLens 1M dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "mnames = ['movie_id', 'title', 'genres']\n",
    "movies = pd.read_table('datasets/movielens/movies.dat', sep='::',\n",
    ".....: header=None, names=mnames)\n",
    "movies[:10]\n",
    "```\n",
    "\n",
    "Adding indicator variables for each genre requires a little bit of wrangling. First, we\n",
    "extract the list of unique genres in the dataset:\n",
    "```\n",
    "all_genres = []\n",
    "for x in movies.genres:\n",
    ".....: all_genres.extend(x.split('|'))\n",
    "genres = pd.unique(all_genres)\n",
    "\n",
    "genres\n",
    "```\n",
    "\n",
    "One way to construct the indicator DataFrame is to start with a DataFrame of all\n",
    "zeros:\n",
    "```\n",
    "zero_matrix = np.zeros((len(movies), len(genres)))\n",
    "dummies = pd.DataFrame(zero_matrix, columns=genres)\n",
    "```\n",
    "Now, iterate through each movie and set entries in each row of dummies to 1. To do\n",
    "this, we use the dummies.columns to compute the column indices for each genre:\n",
    "```\n",
    "gen = movies.genres[0]\n",
    "gen.split('|')\n",
    "\n",
    "dummies.columns.get_indexer(gen.split('|'))\n",
    "```\n",
    "Then, we can use .iloc to set values based on these indices:\n",
    "```\n",
    "for i, gen in enumerate(movies.genres):\n",
    ".....: indices = dummies.columns.get_indexer(gen.split('|'))\n",
    ".....: dummies.iloc[i, indices] = 1\n",
    ".....:\n",
    "```\n",
    "Then, as before, you can combine this with movies:\n",
    "```\n",
    "movies_windic = movies.join(dummies.add_prefix('Genre_'))\n",
    "movies_windic.iloc[0]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy Mechanics\n",
    "\n",
    "Categorizing a dataset and applying a function to each group, whether an aggregation\n",
    "or transformation, is often a critical component of a data analysis workflow. After\n",
    "loading, merging, and preparing a dataset, you may need to compute group statistics\n",
    "or possibly pivot tables for reporting or visualization purposes. pandas provides a\n",
    "flexible groupby interface, enabling you to slice, dice, and summarize datasets in a\n",
    "natural way.\n",
    "\n",
    "Hadley Wickham, an author of many popular packages for the R programming language,\n",
    "coined the term split-apply-combine for describing group operations. In the\n",
    "first stage of the process, data contained in a pandas object, whether a Series, Data‐\n",
    "Frame, or otherwise, is split into groups based on one or more keys that you provide.\n",
    "The splitting is performed on a particular axis of an object. For example, a DataFrame\n",
    "can be grouped on its rows (axis=0) or its columns (axis=1). Once this is done, a\n",
    "function is applied to each group, producing a new value. Finally, the results of all\n",
    "those function applications are combined into a result object. The form of the resulting\n",
    "object will usually depend on what’s being done to the data.\n",
    "\n",
    "![](split-apply-combine.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, here is a small tabular dataset as a DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],\n",
    "                   'key2' : ['one', 'two', 'one', 'two', 'one'],\n",
    "                   'data1' : np.random.randn(5),\n",
    "                   'data2' : np.random.randn(5)})\n",
    "df```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted to compute the mean of the data1 column using the labels from\n",
    "key1. There are a number of ways to do this. One is to access data1 and call groupby\n",
    "with the column (a Series) at key1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "grouped = df['data1'].groupby(df['key1'])\n",
    "grouped```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grouped variable is now a GroupBy object. It has not actually computed anything\n",
    "yet except for some intermediate data about the group key df['key1']. The idea is\n",
    "that this object has all of the information needed to then apply some operation to\n",
    "each of the groups. For example, to compute group means we can call the GroupBy’s\n",
    "mean method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "grouped.mean()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result index has the name 'key1' because the DataFrame column ```df['key1']``` did.\n",
    "\n",
    "If instead we had passed multiple arrays as a list, we’d get something different:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "means = df['data1'].groupby([df['key1'], df['key2']]).mean()\n",
    "means```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we grouped the data using two keys, and the resulting Series now has a **hierarchical\n",
    "index** consisting of the unique pairs of keys observed. We can unstack the hierarchical index as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "means.unstack()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the group keys are all Series, though they could be any arrays of the\n",
    "right length:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])\n",
    "years = np.array([2005, 2005, 2006, 2005, 2006])\n",
    "df['data1'].groupby([states, years]).mean()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently the grouping information is found in the same DataFrame as the data you\n",
    "want to work on. In that case, you can pass column names (whether those are strings,\n",
    "numbers, or other Python objects) as the group keys:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df.groupby('key1').mean()\n",
    "df.groupby(['key1', 'key2']).mean()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed in the first case `df.groupby('key1').mean()` that there is no\n",
    "key2 column in the result. Because `df['key2']` is not numeric data, it is said to be a\n",
    "nuisance column, which is therefore excluded from the result. By default, all of the\n",
    "numeric columns are aggregated, though it is possible to filter down to a subset, as\n",
    "you’ll see soon.\n",
    "\n",
    "Regardless of the objective in using groupby, a generally useful GroupBy method is\n",
    "size, which returns a Series containing group sizes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df.groupby(['key1', 'key2']).size()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating Over Groups\n",
    "The GroupBy object supports iteration, generating a sequence of 2-tuples containing\n",
    "the group name along with the chunk of data. Consider the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for name, group in df.groupby('key1'):\n",
    "    print(name)\n",
    "    print(group)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of multiple keys, the first element in the tuple will be a tuple of key values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for (k1, k2), group in df.groupby(['key1', 'key2']):\n",
    "    print((k1, k2))\n",
    "    print(group)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default groupby groups on axis=0, but you can group on any of the other axes.\n",
    "For example, we could group the columns of our example df here by dtype like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df.dtypes```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "grouped = df.groupby(df.dtypes, axis=1)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the groups like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for dtype, group in grouped:\n",
    "    print(dtype)\n",
    "    print(group)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Column or Subset of Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df.groupby('key1')['data1']\n",
    "df.groupby('key1')[['data2']]```\n",
    "\n",
    "is equivalent to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df['data1'].groupby(df['key1'])\n",
    "df[['data2']].groupby(df['key1'])```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially for large datasets, it may be desirable to aggregate only a few columns. For\n",
    "example, in the preceding dataset, to compute means for just the data2 column and\n",
    "get the result as a DataFrame, we could write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df.groupby(['key1', 'key2'])[['data2']].mean()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object returned by this indexing operation is a grouped DataFrame if a list or\n",
    "array is passed or a grouped Series if only a single column name is passed as a scalar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "s_grouped = df.groupby(['key1', 'key2'])['data2']\n",
    "s_grouped```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "s_grouped.mean()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping with Dicts and Series\n",
    "Grouping information may exist in a form other than an array. Let’s consider another\n",
    "example DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "people = pd.DataFrame(np.random.randn(5, 5),\n",
    "                      columns=['a', 'b', 'c', 'd', 'e'],\n",
    "                      index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])\n",
    "people.iloc[2:3, [1, 2]] = np.nan # Add a few NA values\n",
    "people```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose I have a group correspondence for the columns and want to sum\n",
    "together the columns by group:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "mapping = {'a': 'red', 'b': 'red', 'c': 'blue',\n",
    "           'd': 'blue', 'e': 'red', 'f' : 'orange'}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you could construct an array from this dict to pass to groupby, but instead we\n",
    "can just pass the dict (I included the key 'f' to highlight that unused grouping keys\n",
    "are OK):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "by_column = people.groupby(mapping, axis=1)\n",
    "by_column.sum()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same functionality holds for Series, which can be viewed as a fixed-size mapping:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "map_series = pd.Series(mapping)\n",
    "map_series\n",
    "people.groupby(map_series, axis=1).count()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation\n",
    "Aggregations refer to any data transformation that produces scalar values from\n",
    "arrays. The preceding examples have used several of them, including mean, count,\n",
    "min, and sum. Many common aggregations\n",
    "have optimized implementations. However, you are not limited to only this set of\n",
    "methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use aggregations of your own devising and additionally call any method that\n",
    "is also defined on the grouped object. For example, you might recall that quantile\n",
    "computes sample quantiles of a Series or a DataFrame’s columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "grouped = df.groupby('key1')\n",
    "grouped['data1'].quantile(0.9)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use your own aggregation functions, pass any function that aggregates an array to\n",
    "the aggregate or agg method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def peak_to_peak(arr):\n",
    "    return arr.max() - arr.min()\n",
    "grouped.agg(peak_to_peak)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that some methods like describe also work, even though they are not\n",
    "aggregations, strictly speaking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "grouped.describe()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column-Wise and Multiple Function Application\n",
    "Let’s return to the tipping dataset from earlier examples. After loading it with\n",
    "read_csv, we add a tipping percentage column tip_pct:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tips = pd.read_csv('examples/tips.csv')\n",
    "# Add tip percentage of total bill\n",
    "tips['tip_pct'] = tips['tip'] / tips['total_bill']\n",
    "tips[:6]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you’ve already seen, aggregating a Series or all of the columns of a DataFrame is a\n",
    "matter of using aggregate with the desired function or calling a method like mean or\n",
    "std. However, you may want to aggregate using a different function depending on the\n",
    "column, or multiple functions at once. Fortunately, this is possible to do, which I’ll\n",
    "illustrate through a number of examples. First, I’ll group the tips by day and smoker:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "grouped = tips.groupby(['day', 'smoker'])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for descriptive statistics, you can pass the name of\n",
    "the function as a string:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "grouped_pct = grouped['tip_pct']\n",
    "grouped_pct.agg('mean')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass a list of functions or function names instead, you get back a DataFrame\n",
    "with column names taken from the functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "grouped_pct.agg(['mean', 'std', peak_to_peak])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we passed a list of aggregation functions to agg to evaluate indepedently on the\n",
    "data groups.\n",
    "\n",
    "You don’t need to accept the names that GroupBy gives to the columns; notably,\n",
    "lambda functions have the name '<lambda>', which makes them hard to identify\n",
    "(you can see for yourself by looking at a function’s __name__ attribute). Thus, if you\n",
    "pass a list of (name, function) tuples, the first element of each tuple will be used as\n",
    "the DataFrame column names (you can think of a list of 2-tuples as an ordered\n",
    "mapping):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "grouped_pct.agg([('foo', 'mean'), ('bar', np.std)])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a DataFrame you have more options, as you can specify a list of functions to\n",
    "apply to all of the columns or different functions per column. To start, suppose we\n",
    "wanted to compute the same three statistics for the tip_pct and total_bill\n",
    "columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "functions = ['count', 'mean', 'max']\n",
    "result = grouped['tip_pct', 'total_bill'].agg(functions)\n",
    "result```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the resulting DataFrame has hierarchical columns, the same as you\n",
    "would get aggregating each column separately and using concat to glue the results\n",
    "together using the column names as the keys argument:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "result['tip_pct']```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, a list of tuples with custom names can be passed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ftuples = [('Durchschnitt', 'mean'), ('Abweichung', np.var)]\n",
    "grouped['tip_pct', 'total_bill'].agg(ftuples)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose you wanted to apply potentially different functions to one or more of\n",
    "the columns. To do this, pass a dict to agg that contains a mapping of column names\n",
    "to any of the function specifications listed so far:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "grouped.agg({'tip' : np.max, 'size' : 'sum'})\n",
    "grouped.agg({'tip_pct' : ['min', 'max', 'mean', 'std'],\n",
    "             'size' : 'sum'})```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
